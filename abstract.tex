We address the problem of conditioning probabilistic models on predicates, as a means to express declarative knowledge.
Models conditioned on predicates rarely have a tractable likelihood; sampling from them requires likelihood-free inference.
Existing likelihood-free inference methods
focus on predicates which express observations.
To address a broader class of predicates, we develop an inference procedure called predicate exchange, turns predicates into \emph{soft} predicates that return values in a continuous Boolean algebra: the unit interval with continuous logical connectives.
A soft predicate can serve as a proxy likelihood function in inference,
but introduces an approximation error which depends on a temperature setting.
Zero-temperature predicates yield no error, but are often intractable to condition on.
Higher temperature predicates are easier to sample from, but introduce more error. 
To mitigate this trade-off, we simulate Markov chains at different temperatures and use 
replica exchange to swap between chains.
We implement predicate exchange through a nonstandard execution of a simulation based model, and provide a light-weight tool that can be supplanted on top of existing probabilistic programming formalisms. 
We demonstrate the approach on sequence models of health and inverse rendering.



% 