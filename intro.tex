% !TEX root = icmlsoft.tex

\section{Introduction}

Conditioning in Bayesian inference incorporates observed data into a model.
In a broader sense, conditioning revises a model such that a predicate of uncertain truth becomes a fact.
Conventionally, this predicate is the equality of observable variables to data.
Predicates outside of this class have received significantly less attention, partly because it makes the inference problem significantly more challenging, and partly because conditioning on data accommodates many applications.
Nevertheless, there are many more predicates outside this class than inside; our inability to condition on them is a major limitation.

% Expanding the class of predicates we can condition on, expands the kinds of declarative domain knowledge we can incorporate into generative models.
The ability to condition on a broader class of predicates would enable us to incorporate more kinds of declarative domain knowledge into generative models.
% The kinds of predicates that can be conditioned on determines the kinds of declarative knowledge that can be expressed.
% Knowledge of a domain is often generative in the form of step-by-step causal mechanisms, or declarative in the form of facts. 
% For example, predicates can be used to refine an uninformative prior into one which more closely resembles reality.
For example, probabilistic variants of inverse rendering  \cite{marschner1998inverse,kulkarni2015deep} require a prior distribution over three dimensional scenes.
Some forms of knowledge, such as the fact that rigid bodies do not intersect, are easier to express declaratively as predicates, than constructively in a generative model.
Conditioning on predicates allows us to express what should be true without the burden of specifying how.
% Probability theory fully specifies how , but 

% Declarative facts can be presented as predicates.
% A prior over geometric configurations conditioned on the fact that rigid bodies do not intersect,  will yield better posterior inferences than otherwise, since implausible configurations are eliminated.
% Instead, we would ideally simply condition on it being true, concentrating probability mass on physically plausible geometric configurations, ultimately to yield more accurate posterior inferences in the inverse graphics problem.

Predicates can also represent observations that restrict variables to sets rather than single values.
For example, a medical practitioner may observe that a patient is hypoglycemic, i.e., that their glucose levels have fallen below a critical value.
Given a model over time series of glucose levels \citep{levine2017offline,murata2004probabilistic}, this observation can be realized as a predicate that maps the series to 1 if it falls below the threshold.
Neither hypoglycemia nor any of the infinite number of predicates that can be expressed, need to exist in the generative model apriori.

Probability theory treats conditioning on predicates and concrete observations uniformly, but sampling from models conditioned on most predicates is challenging due to the lack of a tractable likelihood function.
The likelihood function quantifies the extent to which values of latent variables are consistent with observations, and is deemed intractable if it is normalized by intractable integrals or summations.
This can occur, for example, if we condition random variables that are deterministic transformations of other random variables (e.g., the occurrence of hypoglycemia in the example above, or the mean of a collection of variables).	
Alternatively, if the model is generative, i.e. specified as a stochastic simulation, the likelihood is not explicitly available even when the condition is a conventional observation.
The numerous effective likelihood-based sampling \citep{andrieu2003introduction} and variational  \citep{jordan1999introduction, ranganath2014black} methods are inapplicable as a result.

% Even likelihood free inference procedures \cite{} are by themselves unable to accomodate most predicates.
% \todo{WHY?}
% - Focus only on equality
% - Move ethis to discussion
% - Summary statistic
% Inference methods for unnormalized likelihood functions are largely inapplicable to problems when the likelihood is unknown, and vice versa.
% In a similar vein, conditioning on predicates introduces challenges addressed by neither class of methods.

In this paper we present \emph{predicate exchange}:
a likelihood-free method to sample from distributions conditioned on predicates from a broad class.
It is composed of two parts:
\begin{enumerate}
\item \textbf{Predicate Relaxation} constructs soft predicates which return values in a continuous Boolean algebra: the unit interval $[0, 1]$ with continuous logical connectives $\soft{\land}$. $\soft{\lor}$ and $\soft{\neg}$.  Softened predicates approximate their hard counterparts.
\item  \textbf{Replica Exchange} is a Markov Chain Monte Carlo method that simulates Markov chains at different temperatures.  Predicate relaxation is parameterized by a temperature which controls the amount of approximation introduced.  We use replica exchange to draw samples from the unrelaxed model. 
\end{enumerate}

% WTP? Show that soft Boolean is useful for (i) tractability of inference show that rather th	an 
By returning a value in $[0, 1]$ instead of $\{0, 1\}$, a soft predicate quantifies the degree to which values of variables are consistent with the hard predicate.
We concretize this concept in terms of distance: a realization of the model is almost consistent with a predicate if there is another realization that is both consistent with the predicate and close-by with respect to a metric.
% To construct a soft predicate we assume the model has a metric, and informally, a variable values satisfy a predicate more if they are closer to the satisfying set.  define the degree to which a value satisfies a predicate in terms of a distance from an element in $\xp$ that satisfies.

Conventional predicates exist in a Boolean algebra; they can be conjoined, disjoined and negated.
This enables predicates to represent knowledge that has complex Boolean structure.
Continuing the previous example, we may know that a person is \emph{not} hypoglycemic, or that they are hypoglycemic \emph{or} hyperglycemic, or \emph{neither}.
To be able to relax complex predicates, we construct a soft Boolean algebra with continuous counterparts to equality, inequalities and logical connectives.

% Soft predicates resemble likelihood functions; they both return real values that measure the consistency of values of variables.
To perform inference we replace the likelihood term in the Bayesian posterior with a soft predicate.
This yields an \emph{approximate posterior} which we sample from using Markov Chain Monte Carlo.
However, relaxed predicates can still induce complex, multimodal posteriors.
Increasing the temperature smooths the approximate posterior, but also causes it to diverge from the true posterior.
We use replica exchange to mitigate the trade-off. 
Replica exchange simulates high temperature chains -- which explore vasts regions of the sample space -- in parallel with low temperature chains which sample accurately from localized regions.
We augment replica exchange with an accept-reject phase, which allows us to use approximate posteriors to sample from the true posterior when the predicate is of non-zero measure.


% The approximate posterior diverges from the true posterior as an increasing function of the temperature parameter used in predicate relaxation.
% To sample from the true posterior we augment replica exchange -- which simulates several Markov chains at different temperatures in parallel -- with an accept-reject phase. 


% This degree to which a is is determined by a notion of distance.
% In contrast to most distance based inference methods (notably Approximate Bayesian Computation \cite{beaumont2002approximate}), we develop a form of replica exchange Markov Chain Monte Carlo \cite{earl2005parallel} to target inference that is exact in convergence of the chain.
% Predicate relaxation is modulated by temperature such that at zero temperature the relaxed predicate mirrors its hard counter-part, while at maximal temperatures, it is virtually always satisfied.
% Predicate Exchange simulates several Markov chains in parallel at different temperatures.



Predicate exchange addresses a shortcoming of probabilistic programming languages,
which have vastly expanded the class of probabilistic models that can be expressed, but still restrict the kinds of predicates that can be conditioned on to those which result in a tractable likelihood.
In a similar vein to  \cite{wingate2011lightweight} we provide a light-weight implementation that modulates the execution of a stochastic simulation based model to perform inference.
This means predicate exchange is easily incorporated into existing probabilistic languages.
For a concrete implementation, we build predicate exchange into the \textsc{Omega} probabilistic programming language\footnote{\textsc{Omega} is available at \url{http://github.com/zenna/Omega.jl}} \cite{rcd},

% In these cases, a soft predicate may underestimate the degree to which values of latent variables are consistent with the predicate, which can negatively affect inference in practice.

In summary, we:
% In detail, we:

\begin{enumerate}
	% \item Formalize soft conditioning of simulation based  models in measure theoretic probability as a transformation of a probability measure (Section \ref{simmodels}).
	\item Formalize the desiderata for predicate relaxation (Section \ref{predexchange}) and present relaxations of numerical and logical primitive functions.
	% \item Prove that predicate relaxation is preserved under function composition.
	\item Implement predicate exchange as nonstandard execution of a simulation based model (Section \ref{implement}).
	\item Evaluate our approach on examples including inverse rendering and glycemic forecasting (Section \ref{experiments}).
\end{enumerate}


% WTP: Contribution: inference algorithm that supports conditioning on a wider class of propositions
% In this paper we present an algorithm that draws samples from generative models that have been conditioned on predicates belonging to a more general class than observation of data.
% Predicates, when used as black boxes, provide only sparse information -- the constraint is satisfied or it is not -- and the subset of satisfying constraints is typically vanishingly small.
% Our objective is to support conditioning on predicates on spaces for which a natural metric can be defined.
% A metric provides more information a measure of the degree of satisfaction, and allows us.

% WTP: Paper summary
% In summary we address the problem of conditioning on declarative knowledge.
% In more detail:
% \begin{itemize}
% \item We formalize simulation models in measure-theoretic probability as random variables defined on a shared probability space (section X), and define conditioning as a concentration of measure.
% \item We describe our approach to inference, which softens the hard constraints to admit tractable inference in a broader set of scenarios.
% \item  We demonstrate our approach on a number of examples, with experiments on toy data and experiments on medical models by enriching them with declarative knowledge to learn from limited data.
% \end{itemize}