% !TEX root = icmlsoft.tex

\section{Introduction}

% Outline: Conditioning is a general mechanism for expressing declarative knowledge, but so far it has been limited
% - Conditioning allows you to specify what you know to be true without speifying how
% - What is specified using predicates.
% - Dont need to specify how to change generative 
Conditioning in Bayesian inference incorporates observed data into a model.
In a broader sense, conditioning is a mechanism to assert predicates -- which may be true or false -- as true.
An observation is a particular kind of predicate that asserts that a variable is equal to data.
In principle, a predicate can express various kinds of knowledge that need not correspond to the observation of data.
In practice, conditioning on most predicates presents severe challenges to existing inference procedures.


Predicates can express conditions directly on latent variables, rather than observables.
For example, in inverse graphics \cite{kulkarni2015deep} (inferring three dimensional geometry from observed images) the statement ``rigid bodies do not intersect'' is not an observation of data, but is expressible as a predicate.
Conditioning the distribution over latent geometry on this predicate concentrates probability mass on smaller set of hypotheses, ultimately yielding more accurate posterior inferences.

Predicates can also express observations that are more abstract than variables in a model.
In diabetes research for example, probabilistic models have been used to relate physiological factors to glucose levels over time \citep{levine2017offline,murata2004probabilistic}.
Rather than concrete, numerical glucose measurements, a medical practitioner may observe (or be told) that a patient suffers from recurrent hypoglycemia, i.e., that their glucose levels periodically fall below a critical value.
Even if the occurrence of hypoglycemia does not as explicit variable in the model, it can be constructed as a predicate on glucose level variables, and conditioned on to infer the posterior distribution over latent physiological factors.


% ``glucose curves are similar across patients''.	

% It is difficult to define priors of latent geometry that satisfies non-intersection constructively.



% A scientist could define a generative model that captures prior knowledge of how glucose levels evolve over time -- for example, the model may use latent variables to identify when a person eats and the glycemic loads a person consumes, and encode physiological knowledge of how those meals will affect glucose levels.
% The scientist could also condition the model on observations of glucose measurements of a given patient and use existing inference algorithms to sample from the posterior distributions over latent variables.
% However, there are various pieces of declarative knowledge the scientist may possess which are difficult or impossible to encode in this form.
% For instance, she may know that ``human glucose curves are similar across patients'',   is surprisingly challenging even in the most expressive probabilistic programming systems. 
% Rather than observations, these propositions are facts which condition the prior. These propositions are not observations.


% WTP: What are simulation based models, what is bayesian inference in them.  Why is it necessary. 
% Inference on models conditioned on more general predicates yield challenging inference problems, especially when the models are represented as stochastic simulators.
% For many phenomena in the world, we can build models that 
% attempt to simulate how they evolve. One purpose of 
% these models is to draw samples from unobserved variables 
% conditional on the output of the simulation, i.e, perform
% posterior inference.
% Most algorithms which draw samples from posterior distributions rely on
% the likelihood function, which quantifies the extent to which values of 
% unobserved variables are consistent with observations.
% Simulation models, however leave the likelihood functions implicit
% and difficult to derive.
% %This can occur, for example, when non-latent variables are unobserved, since they must be margainalized out, or when transformed densities are conditioned.
% Simulation based models can apply arbitrary, non-injective transformations to their input, and hence induce intractable likelihoods for any or all of these reasons.

% A less explored cause of intractability is propositions that are not observations in the conventional sense.
% To observe data $x$ as the output of a simulation model $X$ means to condition the model on the proposition $X = x$.
% There are several propositions that do not conform to this structure, which we would nevertheless like to condition our models on.
% These propositions may impose constraints on latent variables, or even jointly on latent and observations.
% For example.
% They could also be observational but more abstract than concrete data.
% For example,.
% All such statements can be enforced through conditioning, but generally the resulting likelihood will be intractable or may not exist,


% Problems of inference, on the other hand, are typically anti-causal: having observed the result of the simulation we wish to infer the causes.



% lack succinct equations governing their behavior but nevertheless understand the causal mechanisms by which they evolve.
% Such phenomena can be simulated, which means to execute a program and associate states which arise in its execution with entities in the domain.
% as an algorithm which solves the equations which govern it. Values, states, and transitions in the execution of the algorithm correspond to entities in the domain.
% This execution generally follows the causal direction of the domain in the sense that if $a$ causes $b$, $b$ is computed from $a$ in simulation.
% Problems of inference, on the other hand, are typically anti-causal: having observed the result of the simulation we wish to infer the causes.



% WTP: What are likelihood functions, why are they use useful, and why are they intractable
% The Bayesian approach to inference is to first quantify all uncertainty with probability distributions, and as a result uniquely specify the posterior distribution given observed values.
% % Bayesian inference is a principled framework for inferring probable causes of observed values, but relies on terms which are often intractable.
% Most algorithms which draw samples from posterior distributions rely on
% % Bayes theorem dictates that the posterior distribution over latent variables given observations is is proportional to the product of the likelihood function and prior.
% the likelihood function, which quantifies the extent to which values of latent variables are consistent with observations.
% Simulation models explicitly represent the processes which generate data, but leave the likelihood functions implicit. 
% The likelihood  difficult to derive from a simulation model and is often intractable to evaluate.
% This can occur, for example, when non-latent variables are unobserved, since they must be margainalized out, or when transformed densities are conditioned.
% Simulation based models can apply arbitrary, non-injective transformations to their input, and hence induce intractable likelihoods for any or all of these reasons.

% Morever, approximate inference algorithm 

% % A less explored cause of intractability occurs when we condition on propositions that are not observations of data.
% For example, if $X$ and $Y$ are normally distributed random variables, then conditioning on $X = Y$ does not permit a density function, let alone a tractable one.
% Conditioning on inequalities such $X > 4$, transformations such $X^2  = 0$, and logical formulas such as  $X = 3 \lor X = 5$ falls outside the domain of virtually all conditional sampling algorithms.
% % As a consequence, conditioning in practice is significantly more limited than its theoretical counterpart.

% The value of probabilistic programming systems hinges on how easily a practitioner can encode domain knowledge into a model. Existing probabilistic programming systems support two main mechanisms for encoding knowledge: implicitly in the generative model, and explicitly by conditioning on observations. There are many contexts, however, for which neither of these two mechanisms are sufficient to encode the knowledge that practitioners have about a process.

% WTP: Implications of the Problem


% WTP: Non-obvious benefits of solving the problem:  Prior conditioning.
% Declarative knowledge has the potential to serve as the bridge between classical parametric models, and recent trend towards nonparameric or highly parameterized families such as deep neural networks.
% For example, inverse graphics attempts to infer the three dimensional scene (geometry, lights, camera, etc) which caused an observed image.
% A Bayesian formulation of the problem requires a prior distributions over scenes.
% Simple parametric stand little chance of capturing the complexity of real world scenes.

% Outline: Non-observational predicates induce intractable likelihoods
% and are largely unsupported by current methods of inference
Several effective sampling  \citep{andrieu2003introduction} and variational  \citep{jordan1999introduction, ranganath2014black} approaches to inference require only a black-box likelihood function, i.e., one evaluable on arbitrary input.
The likelhood function quantifies the extent to which values of latent variables are consistent with observations. 
However, models conditioned on predicates tend to have likelihood functions that are intractable to compute or incompatible with most inference methods.
For example, conditioning random variables that do not explicitly appear in the data generating process (e.g., the product of variables) can result in likelihoods that are normalized by intractable integrals.	
% For example, given three variables, it is possible to condition on the sum
% of the variables being positive; this sum is never an explicit random variable.
In many cases, especially in simulation based models, the likelihood function is implicit and hence unavailable even when the condition is a conventional observation.

In this paper we present \emph{predicate exchange}:
a method to sample from distributions conditioned on predicates from a broad class.
It is composed of two parts:
\begin{enumerate}
\item \textbf{Predicate Relaxation} transforms a probabilistic model such that predicates it is conditioned on take values in the unit interval $[0, 1]$ rather than $\{0, 1\}$.
\item  \textbf{Replica Exchange} simulates several Markov chains  conditioned on predicates relaxed to different degrees in parallel, and in doing so is able to draw exact samples. 
\end{enumerate}

Predicate relaxation meets two desiderata.
First, values in $[0, 1]$ denote a degree to which the predicate is satisfied.
This allows a soft predicate to serve as an approximate likelihood, and opens up the use of likelihood-based inference procedures.
Second, values in $[0, 1]$ form a complete Boolean algebra, composed of conjunction, disjunction and negation.
This is crucial because knowledge often has a Boolean structure.
We may know that a proposition is not true, that two propositions simultaneously hold, or that one implies the other.
% Few likelihood-based likelihood-free approaches to inference offer effective means to condition non-trivial models on compound predicates.

% This degree to which a is is determined by a notion of distance.
% In contrast to most distance based inference methods (notably Approximate Bayesian Computation \cite{beaumont2002approximate}), we develop a form of replica exchange Markov Chain Monte Carlo \cite{earl2005parallel} to target inference that is exact in convergence of the chain.
Predicate relaxation is modulated by temperature such that at zero temperature the relaxed predicate mirrors its hard counter-part, while at maximal temperatures, it is virtually always satisfied.
Predicate Exchange simulates several Markov chains in parallel at different temperatures.

For implementation we take two approaches.
For completeness, we define predicate relaxation as graph transformation to a form of graphical model. 
In addition, to allow predicate exchange to be incorporated into different modeling formalisms and frameworks, mirroring  \cite{wingate2011lightweight}, we provide a light-weight implementation that implements predicate exchange by modulating the execution of stochastic simulation based model. 

Our approach comes with certain limitations.
Predicates which indicate sets of zero measure -- typically equalities on continuous variables -- are problematic because X.
In these cases PE samples at a temperature strictly greater than zero.
These samples are approximate.

% zero. However, mixing may be slow since condition on general predicates
% encodes the class of decision problems.
% Hard valued predicates do not have scaling issues as they are either zero or one valued, but soft valued predicates can have issues. Our procedure
% to convert from hard valued predicates to soft valued ones
% tries to ensure that each predicate gets satisfied with equal
% probability at a fixed temperature. This equal weighting 
% avoids bias for one predicate or another due to the choice of sampling.

% We build our inference algorithm into a system.
% \emph{Omega} provides an to define a generative model and declare
% predicates about it. Given these definitions, \emph{Omega} automatically
% performs inference using our soften-contraint replica exchange algorithm.
% We test \emph{Omega} on a simulated example with truncated Gaussians,
% an object rendering model, a physiological model, and xxx 

In summary we address the problem of conditioning probabilistic models on predicates as a means to express declarative knowledge.
In detail, we:

\begin{enumerate}
	\item Formalize simulation based probabilistic models in measure theoretic probability, and conditioning as the imposition of constraints (section \ref{simmodels}).
	\item Present an algorithm based on replica-exchange Markov Chain Monte Carlo (section \ref{}) which draws exact samples for positive measure predicates. 
	\item Formalize predicate relaxation as a transformation to the syntax   the syntax and semantics of a minimal statistical language 
	\item Implement these concepts in system for building models, predicates, and doing inference.
	\item Evaluate our approach on representative examples, and demonstrate case studies including enriching medical models with limited data.
\end{enumerate}


% WTP: Contribution: inference algorithm that supports conditioning on a wider class of propositions
% In this paper we present an algorithm that draws samples from generative models that have been conditioned on predicates belonging to a more general class than observation of data.
% Predicates, when used as black boxes, provide only sparse information -- the constraint is satisfied or it is not -- and the subset of satisfying constraints is typically vanishingly small.
% Our objective is to support conditioning on predicates on spaces for which a natural metric can be defined.
% A metric provides more information a measure of the degree of satisfaction, and allows us.

% WTP: Paper summary
% In summary we address the problem of conditioning on declarative knowledge.
% In more detail:
% \begin{itemize}
% \item We formalize simulation models in measure-theoretic probability as random variables defined on a shared probability space (section X), and define conditioning as a concentration of measure.
% \item We describe our approach to inference, which softens the hard constraints to admit tractable inference in a broader set of scenarios.
% \item  We demonstrate our approach on a number of examples, with experiments on toy data and experiments on medical models by enriching them with declarative knowledge to learn from limited data.
% \end{itemize}