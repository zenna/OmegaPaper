\section{Introduction}

% WTP: Simulation based models are plentiful, but do not permit tractable likelihoods 
For a variety of phenomena in physics, biology, simulation is unique in its ability to capture the 

Simulation models can accurately represent many phenomena in physics, biology, and artificial intelligence but are resistant to analytical treatment.
This presents both promise and peril for Bayesian inference problems; often simulation is uniquely capable of expressing complex probabilistic models

The expressiveness of simulation based models renders them to several analysis.
Bayesian inference offers a principled solution to this problem 
by quantifying all unknown values as probability distributions.
The posterior distribution expresses what we should believe having observed the output of the simulation, and is uniquely determined by the rules of probability, eliminating any ambiguity.
In practice however, the vast majority of tractable methods for Bayesian inference depend upon a tractable likelihood function.


% Running the simulation then requires random numbers to simulate sampling from the distribution.
% In such models, it is generally easy to simulate the model in the forward direction, but the objective is the inverse: to sample values of parameters that are consistent with observations.
% - Observations are constraints of the form X = x, where X is n unknown quantity and and x is data


% % WTP: The Problem: There are several kinds of statements we would like to condition our probabilistic models on, but no inference algorithm supports them.
% Samples are a fundamental representation of uncertainty.
% They are applicable to probability distributions of any kind, and several other properties such as expectation and variance can be approximated from them.
% Whereas the class of generative models for which tractable sampling strategies exist has grown to include those with infinite dimensions \cite{}, highly flexible nonlinear models, the kinds of statement we can condition a model on remains severely restricted.

In the Bayesian paradigm, to observe the output of a simulation model is to condition the model on a constraint of the form $X = x$, where $X$ is a random variable and $x$ is a concrete value.
Sampling from conditioned models is considered intractable if the likelihood function --  which quantifies the extent to which random variables in the model are consistent with observations -- is intractable to compute.
A likelihood function is intractable if it is implicitly defined by an integral or intractable partition function.
This can occur for a variety of reasons, for example when there are unobserved variables, when deterministic transformations of densities are conditioned, or when primitive distributions in the model have intractable densities.
Simulation based models apply arbitrary, often nonlinear transformations to their input, and hence only in limited cases permit a tractable likelihood.

% WTP: Underappreciate Reason for Problem: Conditioning on observations
Conditioning is more general than observation.
In modern probability theory it is an operation that restricts a model to be consistent with a statement.
A less explored cause of intractability occurs when we condition on statements that are not observations of data.
For example, if $X$ and $Y$ are normally distributed random variables, then conditioning on $X = Y$ does not permit a density function, let alone a tractable one.
Conditioning on inequalities such $X > 4$, transformations such $X^2  = 0$, and logical operations such as  $X = 3 \lor X = 5$ falls outside the domain of virtually all conditional sampling algorithms.
As a consequence, conditioning in practice is significantly more limited than its theoretical counterpart.

% The value of probabilistic programming systems hinges on how easily a practitioner can encode domain knowledge into a model. Existing probabilistic programming systems support two main mechanisms for encoding knowledge: implicitly in the generative model, and explicitly by conditioning on observations. There are many contexts, however, for which neither of these two mechanisms are sufficient to encode the knowledge that practitioners have about a process.

% WTP: Implications of the Problem

Semantically, conditioning is a mechanism to express declarative knowledge.
It allows us to assert facts we know to be true without specifying the means by which they are true.
Limiting the statements one can condition on to those with tractable likelihoods severely restricts the kinds of knowledge that we can express.
For example, consider the problem of modeling the evolution of glucose levels over time~\citep{levine2017offline}.
A scientist could define a generative model that captures prior knowledge of how glucose levels evolve over time -- for example, the model may use latent variables to identify when a person eats and the glycemic loads a person consumes, and encode some physiological knowledge of how those meals will affect glucose levels.
The scientist could also condition the model on observations of glucose measurements of a given patient and use existing inference algorithms to sample from the posterior distributions over latent variables.
However, there are various pieces of declarative knowledge the scientist may possess which are difficult or impossible to encode in this form.
For instance, she may know that ``human glucose curves are similar across patients'' is surprisingly challenging even in the most expressive probabilistic programming systems. 
Rather than observations, these statements are facts which condition the prior.These statements are not observations; 

% WTP: Non-obvious benefits of solving the problem:  Prior conditioning.
The ability to express declarative knowledge through conditioning can serve as the bridge between classical Bayesian modeling which emphasizes interpretability, and recent trends of using highly parameterized families such as deep neural networks.
For example, inverse graphics attempts to infer the three dimensional scene properties (geometry, lights, camera position) which caused an observed image.
This requires a prior distributions over scenes.



% WTP: Contribution: inference algorithm that supports conditioning on a wider class of statements
In this paper we present an algorithm that draws samples from generative models that have been conditioned on predicates belonging to a more general class than observation of data.
Predicates, when used as black boxes, provide only sparse information -- the constraint is satisfied or it is not -- and the subset of satisfying constraints is typically vanishingly small.
Our objective is to support conditioning on predicates on spaces for which a natural metric can be defined.
A metric provides more information a measure of the degree of satisfaction, and allows us.

% WTP: Paper summary
The rest of the paper proceeds as follows:
\begin{itemize}
\item We formalize simulation models in measure-theoretic probability as random variables defined on a shared probability space (section X), and define conditioning as a concentration of measure.
\item We describe our approach to inference, which softens the hard constraints to admit tractable inference in a broader set of scenarios.
\item  We demonstrate our approach on a number of examples, with experiments on toy data and experiments on medical models by enriching them with declarative knowledge to learn from limited data.
\end{itemize}