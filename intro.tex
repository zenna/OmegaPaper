% !TEX root = icmlsoft.tex

\section{Introduction}

Conditioning in Bayesian inference incorporates observed data into a model.
In a broader sense, conditioning revises a model such that a predicate of uncertain truth becomes a fact.
Conventionally, this predicate is the equality of observable variables to data.
Predicates outside of this class have received significantly less attention, partly because it makes the inference problem significantly more challenging, and partly because conditioning on data has proven sufficient to accommodate many applications.
Nevertheless, there are many more predicates outside this class than inside; our inability to condition on them is a major limitation.

The ability to condition on predicates would enable us to incorporate declarative domain knowledge into generative models.
% The kinds of predicates that can be conditioned on determines the kinds of declarative knowledge that can be expressed.
% Knowledge of a domain is often generative in the form of step-by-step causal mechanisms, or declarative in the form of facts. 
% For example, predicates can be used to refine an uninformative prior into one which more closely resembles reality.
For example, probabilistic variants of inverse rendering  \cite{marschner1998inverse,kulkarni2015deep} require a prior distribution over three dimensional scenes.
Some forms of knowledge are are easily expressed generatively, such as the fact that light sources are generally above \cite{lightfromabove}.
Others are much easier to express declaratively, such as the fact that rigid bodies do not intersect.
De	clarative facts can be presented as predicates.
% A prior over geometric configurations conditioned on the fact that rigid bodies do not intersect,  will yield better posterior inferences than otherwise, since implausible configurations are eliminated.
% Instead, we would ideally simply condition on it being true, concentrating probability mass on physically plausible geometric configurations, ultimately to yield more accurate posterior inferences in the inverse graphics problem.

Predicates can also represent observations that restrict variables to sets rather than single values.
For example, a medical practitioner may observe that a patient is hypoglycemic, i.e., that their glucose levels have fallen below a critical value.
Given a model relating physiological factors to glucose levels \citep{levine2017offline,murata2004probabilistic}, this observation can be realized as a predicate that maps a time series of glucose values to true if and only if it falls below the threshold.
Crucially, the occurrence of hypoglycemia does not need to appear as an explicit variable in the model.

Sampling from models conditioned on most predicates is challenging due to the lack of a tractable likelihood function.
The likelihood function quantifies the extent to which values of latent variables are consistent with observations, and is deemed intractable if it is normalized by intractable integrals or summations.
This can occur, for example, if we condition random variables that are deterministic transformations of other random variables (e.g., the occurrence of hypoglycemia in the example above, or the mean of a collection of variables).	
Alternatively, if the model is generative, i.e. specified as a stochastic simulation, the likelihood is not explicitly available even when the condition is a conventional observation.
The numerous effective likelihood-based sampling \citep{andrieu2003introduction} and variational  \citep{jordan1999introduction, ranganath2014black} methods are often inapplicable as a result.

% Even likelihood free inference procedures \cite{} are by themselves unable to accomodate most predicates.
% \todo{WHY?}
% - Focus only on equality
% - Move ethis to discussion
% - Summary statistic
% Inference methods for unnormalized likelihood functions are largely inapplicable to problems when the likelihood is unknown, and vice versa.
% In a similar vein, conditioning on predicates introduces challenges addressed by neither class of methods.

In this paper we present \emph{predicate exchange}:
a likelihood-free method to sample from distributions conditioned on predicates from a broad class.
It is composed of two parts:
\begin{enumerate}
\item \textbf{Predicate Relaxation} constructs soft predicates which return values in a continuous Boolean algebra: the unit interval $[0, 1]$ with continuous logical connectives $\soft{\land}$. $\soft{\lor}$ and $\neg$.
\item  \textbf{Replica Exchange} is a Markov Chain Monte Carlo method originating in statistical physics, that simulates Markov chains at different temperatures.  Predicate relaxation is parameterized by a temperature which controls the amount of approximation introduced.  We use replica exchange to draw samples from the unrelaxed model. 
\end{enumerate}

% WTP? Show that soft Boolean is useful for (i) tractability of inference show that rather th	an 
By returning a value in $[0, 1]$ instead of $\{0, 1\}$, a soft predicate quantifies the degree to which values of variables are consistent with the hard predicate.
We reify this notion of graded predicate satisfaction in terms of distance; a realization of the model is almost consistent with a predicate if there is another realization that is both consistent with the predicate and close-by with respect to a metric.
% To construct a soft predicate we assume the model has a metric, and informally, a variable values satisfy a predicate more if they are closer to the satisfying set.  define the degree to which a value satisfies a predicate in terms of a distance from an element in $\xp$ that satisfies.


Predicates exist in a Boolean algebra; they can be conjoined, disjoined and negated.
This allows predicates to represent knowledge that has complex Boolean structure.
Continuing the previous example, we may know that a person does \emph{not} have hypoglycemia, or that they have hypoglycemia \emph{or} hyperglycemia, or \emph{neither}.
To accommodate this, we construct a soft Boolean algebra with continuous counterparts to equality, inequalities and logical connectives.

Soft predicates resemble likelihood functions; they both return real values that measure the consistency of values of variables.
To perform inference we take this analogy literally by replacing the likelihood term in a posterior with a soft predicate.
This yields an \emph{approximate posterior} which can be sampled from using existing likelihood based inference procedures.
The approximate posterior diverges from the true posterior as an increasing function of the temperature parameter used in predicates relaxation.
To sample from the true posterior we augment replica exchange -- which simulates several Markov chains at different temperatures in parallel -- with an accept-reject phase. 


% This degree to which a is is determined by a notion of distance.
% In contrast to most distance based inference methods (notably Approximate Bayesian Computation \cite{beaumont2002approximate}), we develop a form of replica exchange Markov Chain Monte Carlo \cite{earl2005parallel} to target inference that is exact in convergence of the chain.
% Predicate relaxation is modulated by temperature such that at zero temperature the relaxed predicate mirrors its hard counter-part, while at maximal temperatures, it is virtually always satisfied.
% Predicate Exchange simulates several Markov chains in parallel at different temperatures.



Predicate exchange addresses a shortcoming of probabilistic programming languages,
which have vastly expanded the class of probabilistic models that can be expressed, but still restrict the kinds of predicates that can be conditioned on to those which result in a tractable likelihood.
In a similar vein to  \cite{wingate2011lightweight} we provide a light-weight implementation that modulates the execution of a stochastic simulation based model to perform inference.
This means predicate exchange is easily incorporated into existing probabilistic languages. 

Our approach comes with certain limitations.
Equality conditions on continuous variables indicate sets of zero measure.
This is problematic because the probability of proposing a satisfying state in a Markov chain becomes zero.
In these cases predicate exchange must sample at a minimum temperature strictly greater than zero, which is approximate.
Another limitation occurs if a predicate has branches (e.g., if-then-else statements) such that the execution-path taken depends on uncertainty in the model.
Such branches make it more difficult to estimate how close values of variables are to satisfying a predicate.
% In these cases, a soft predicate may underestimate the degree to which values of latent variables are consistent with the predicate, which can negatively affect inference in practice.

In summary, we:
% In detail, we:

\begin{enumerate}
	% \item Formalize soft conditioning of simulation based  models in measure theoretic probability as a transformation of a probability measure (Section \ref{simmodels}).
	\item Formalize the desiderata for predicate relaxation (Section \ref{predexchange}) and present relaxations of numerical and logical primitive functions.
	% \item Prove that predicate relaxation is preserved under function composition.
	\item Implement predicate exchange as nonstandard execution of a simulation based model (Section \ref{implement}).
	\item Evaluate our approach on examples, including case studies in inverse rendering and glycemic forecasting (Section \ref{experiments}).
\end{enumerate}


% WTP: Contribution: inference algorithm that supports conditioning on a wider class of propositions
% In this paper we present an algorithm that draws samples from generative models that have been conditioned on predicates belonging to a more general class than observation of data.
% Predicates, when used as black boxes, provide only sparse information -- the constraint is satisfied or it is not -- and the subset of satisfying constraints is typically vanishingly small.
% Our objective is to support conditioning on predicates on spaces for which a natural metric can be defined.
% A metric provides more information a measure of the degree of satisfaction, and allows us.

% WTP: Paper summary
% In summary we address the problem of conditioning on declarative knowledge.
% In more detail:
% \begin{itemize}
% \item We formalize simulation models in measure-theoretic probability as random variables defined on a shared probability space (section X), and define conditioning as a concentration of measure.
% \item We describe our approach to inference, which softens the hard constraints to admit tractable inference in a broader set of scenarios.
% \item  We demonstrate our approach on a number of examples, with experiments on toy data and experiments on medical models by enriching them with declarative knowledge to learn from limited data.
% \end{itemize}