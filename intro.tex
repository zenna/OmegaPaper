% !TEX root = icmlsoft.tex

\section{Introduction}

Conditioning in Bayesian inference incorporates observed data into a model.
In a broader sense, conditioning revises a model such that a yes/no question (a predicate) is resolved to a true proposition (a fact).
In the case of an observation, the question of whether a variable is equal to the observed value, changes from a predicate of uncertain truth, to a fact, once it is observed.

% 
% 
% 
% 
% ``glucose curves are similar across patients''.	
% 
% It is difficult to define priors of latent geometry that satisfies non-intersection constructively.
% A scientist could define a generative model that captures prior knowledge of how glucose levels evolve over time -- for example, the model may use latent variables to identify when a person eats and the glycemic loads a person consumes, and encode physiological knowledge of how those meals will affect glucose levels.
% The scientist could also condition the model on observations of glucose measurements of a given patient and use existing inference algorithms to sample from the posterior distributions over latent variables.
% However, there are various pieces of declarative knowledge the scientist may possess which are difficult or impossible to encode in this form.
% For instance, she may know that ``human glucose curves are similar across patients'',   is surprisingly challenging even in the most expressive probabilistic programming systems. 
% Rather than observations, these propositions are facts which condition the prior. These propositions are not observations.
In principle, a predicate can be used to declare any fact about a domain, not only the observation of data.
% Predicates can be used to update a model to adhere to known facts about a domain, without the burden of specifying how to revise the model.
For example, inverse rendering  \cite{marschner1998inverse,kulkarni2015deep} aims to infer three dimensional geometry from observed images.
The proposition ``rigid bodies do not intersect'' is a predicate on latent configurations of geometry.
To manually revise a model to constructively adhere to this fact ranges between inconvenient and infeasible.
% Instead, we would ideally simply condition on it being true, concentrating probability mass on physically plausible geometric configurations, ultimately to yield more accurate posterior inferences in the inverse graphics problem.
Predicates can be used to express facts that are observations, but that are more abstract than variables	 in a model.
In diabetes research, probabilistic models have been used to relate physiological factors to glucose levels over time \citep{levine2017offline,murata2004probabilistic}.
Rather than concrete, numerical glucose measurements, a medical practitioner may observe (or be told) that a patient suffers from recurrent hypoglycemia, i.e., that their glucose levels periodically fall below a critical value.
Even if the occurrence of hypoglycemia does not appear as an explicit variable in the model, it could be constructed as a predicate on glucose levels, and conditioned on to infer the posterior distribution over latent physiological factors.
In practice, sampling from models conditioned on most predicates presents severe challenges to existing inference procedures.


Several effective sampling  \citep{andrieu2003introduction} and variational  \citep{jordan1999introduction, ranganath2014black} approaches to inference require only a black-box likelihood function, i.e., one evaluable on arbitrary input.
The likelihood function quantifies the extent to which values of latent variables are consistent with observations. 
However, most models conditioned on most predicates have likelihood functions that are intractable to compute or unknown.
For example, conditioning random variables that are deterministic transformations of other random variables (e.g., the presence of hypoglycemia in the example above, or the mean of a collection of variables) often results in likelihoods that are normalized by intractable integrals.	
% For example, given three variables, it is possible to condition on the sum
% of the variables being positive; this sum is never an explicit random variable.
In other cases, the likelihood function is implicit to a generative process, rather than explicitly specified, and hence unavailable even when the condition is a conventional observation.
Inference methods for an unnormalized likelihood functions are largely inapplicable to problems  when the likelihood is unknown, and vice versa.
In a similar vein, conditioning on predicates introduces challenges unaddressed by either.

In this paper we present predicate exchange:
a likelihood-free method to sample from distributions conditioned on predicates from a broad class.
It is composed of two parts:
\begin{enumerate}
\item \textbf{Predicate Relaxation} constructs soft predicates which return values in a continuous Boolean algebra: the unit interval $[0, 1]$ with continuous logical connectives $\soft{\land}$. $\soft{\lor}$ and $\neg$.
\item  \textbf{Replica Exchange} simulates several Markov chains of a model at different temperatures.  Temperature is a parameter of predicate relaxation which controls the amount of approximation introduced.  We adapt standard replica exchange to draw samples that are asymptotically exact from the unrelaxed model. 
\end{enumerate}

% WTP? Show that soft Boolean is useful for (i) tractability of inference show that rather th	an 
By returning a value in $[0, 1]$ instead of $\{0, 1\}$, a soft predicate quantifies the extent to which values of latent variables are consistent with the predicate.
This allows it to serve a role similar to a likelihood function, and opens up the use of likelihood-based inference procedures.
Conditioning is conventionally formalized as a restriction of a probability space to an event.
To formalize soft conditioning, we generalize this to transformation of probability measure.

Orthogonally, we embed $]0, 1]$ in a Boolean algebra to support the expression of domain knowledge of composite Boolean structure.
Continuing the previous example, we may know that a person does \emph{not} have hypoglycemia, or that they have hypoglycemia \emph{or} hyperglycemia, or \emph{neither}.
% Few likelihood-based likelihood-free approaches to inference offer effective means to condition non-trivial models on compound predicates.

% This degree to which a is is determined by a notion of distance.
% In contrast to most distance based inference methods (notably Approximate Bayesian Computation \cite{beaumont2002approximate}), we develop a form of replica exchange Markov Chain Monte Carlo \cite{earl2005parallel} to target inference that is exact in convergence of the chain.
% Predicate relaxation is modulated by temperature such that at zero temperature the relaxed predicate mirrors its hard counter-part, while at maximal temperatures, it is virtually always satisfied.
% Predicate Exchange simulates several Markov chains in parallel at different temperatures.



Predicate exchange is motivated by probabilistic programming languages, which have vastly expanded the class of probabilistic models that can be expressed,, but still heavily restrict the kinds of predicates that can be conditioned on.
Rather than introduce a new language or modeling formalism, we mirror   \cite{wingate2011lightweight} and provide a light-weight implementation modulates the execution of a stochastic simulation based model to perform inference.
This means predicate exchange is easily incorporated into most frameworks. 

Our approach comes with certain limitations.
Equality conditions on continuous variables indicate sets of zero measure.
This is problematic because the probability of proposing a satisfying state in a Markov chain becomes zero.
In these cases predicate exchange must sample at a minimum temperature strictly greater than zero, which is approximate.
Another limitation occurs if a predicate has branches (e.g., if-then-else statements) which depend on uncertainty in the model.
% Since all possible paths through a program are visited, our measure of is necessarily an estimate.

In summary, we:
% In detail, we:

\begin{enumerate}
	\item Formalize soft conditioning of simulation based  models in measure theoretic probability as a transformation of a probability measure (Section \ref{simmodels}).
	\item Motivate predicate relaxation (Section \ref{predexchange}), and provide a complete continuous Boolean algebra.
	\item Implement predicate exchange as nonstandard execution of a simulation based model (Section \ref{implement}).
	\item Evaluate our approach on examples, including a case study in glycemic forecasting (Section \ref{experiments}).
\end{enumerate}


% WTP: Contribution: inference algorithm that supports conditioning on a wider class of propositions
% In this paper we present an algorithm that draws samples from generative models that have been conditioned on predicates belonging to a more general class than observation of data.
% Predicates, when used as black boxes, provide only sparse information -- the constraint is satisfied or it is not -- and the subset of satisfying constraints is typically vanishingly small.
% Our objective is to support conditioning on predicates on spaces for which a natural metric can be defined.
% A metric provides more information a measure of the degree of satisfaction, and allows us.

% WTP: Paper summary
% In summary we address the problem of conditioning on declarative knowledge.
% In more detail:
% \begin{itemize}
% \item We formalize simulation models in measure-theoretic probability as random variables defined on a shared probability space (section X), and define conditioning as a concentration of measure.
% \item We describe our approach to inference, which softens the hard constraints to admit tractable inference in a broader set of scenarios.
% \item  We demonstrate our approach on a number of examples, with experiments on toy data and experiments on medical models by enriching them with declarative knowledge to learn from limited data.
% \end{itemize}