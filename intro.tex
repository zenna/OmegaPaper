% !TEX root = icmlsoft.tex

\section{Introduction}

Conditioning in Bayesian inference incorporates observed data into a model.
In a broader sense, conditioning revises a model such that a predicate is resolved to a true proposition (a fact).
To illustrate, consider estimating a parameter $\Theta$ given an observation $x$.  Bayes rule dictates:
$$
p(\theta \mid X = x) \propto p(x \mid \theta) \cdot p(\theta)
$$
$X = x$ is a predicate, a Boolean valued function, which takes value 0 or 1 depending on uncertaitny on the model.
Conditioning on a predicate forces it to be true.
There are many predicates are easily expressed, such $X$
 % Conditioning on an observation, for instance, resolves the question of whether a variable is equal to a particular value, from a predicate of uncertain truth, to a fact.

Predicates can be used to declare any fact about a domain. For example, inverse rendering  \cite{marschner1998inverse,kulkarni2015deep} aims to infer three dimensional geometry from observed images.
A prior over geometric configurations conditioned on the fact that rigid bodies do not intersect,  will yield better posterior inferences than otherwise, since implausible configurations are eliminated.
% Instead, we would ideally simply condition on it being true, concentrating probability mass on physically plausible geometric configurations, ultimately to yield more accurate posterior inferences in the inverse graphics problem.
Predicates can also express facts that are observations, but that are more abstract than variables	 in a model.
In diabetes research, probabilistic models have been used to relate physiological factors to glucose levels over time \citep{levine2017offline,murata2004probabilistic}.
Rather than concrete, numerical glucose measurements, a medical practitioner may observe (or be told) that a patient suffers from recurrent hypoglycemia, i.e., that their glucose levels periodically fall below a critical value.
Even if the occurrence of hypoglycemia does not appear as an explicit variable in the model, it could be constructed as a predicate that maps a time series of glucose values to true if and if it repeatedly rises below and above the threshold.
In principle, these facts can be incorporated into the model directly through conditioning.	
In practice, sampling from models conditioned on most predicates presents severe challenges to existing inference procedures.


Several effective sampling  \citep{andrieu2003introduction} and variational  \citep{jordan1999introduction, ranganath2014black} approaches to inference require only a black-box likelihood function, i.e., one evaluable on arbitrary input.
The likelihood function quantifies the extent to which values of latent variables are consistent with observations. 
However, most models conditioned on most predicates have likelihood functions that are either intractable to compute or unknown.
For example, conditioning random variables that are deterministic transformations of other random variables (e.g., the presence of hypoglycemia in the example above, or the mean of a collection of variables) often results in likelihoods that are normalized by intractable integrals or summations.	
Alternatively, if the model is generative, i.e. specified as a stochastic simulation, the likelihood is not explicitly available even when the condition is a conventional observation.
Inference methods for an unnormalized likelihood functions are largely inapplicable to problems  when the likelihood is unknown, and vice versa.
In a similar vein, conditioning on predicates introduces challenges unaddressed by either.

In this paper we present \emph{predicate exchange}:
a likelihood-free method to sample from distributions conditioned on predicates from a broad class.
It is composed of two parts:
\begin{enumerate}
\item \textbf{Predicate Relaxation} constructs soft predicates which return values in a continuous Boolean algebra: the unit interval $[0, 1]$ with continuous logical connectives $\soft{\land}$. $\soft{\lor}$ and $\neg$.
\item  \textbf{Replica Exchange} is a Markov Chain Monte method originating in statistical physics, that simulates Markov chains at different temperatures.  Predicate relaxation is parameterized by a temperature which controls the amount of approximation introduced.  We use replica exchange to draw samples that are asymptotically exact from the unrelaxed model. 
\end{enumerate}

% WTP? Show that soft Boolean is useful for (i) tractability of inference show that rather th	an 
By returning a value in $[0, 1]$ instead of $\{0, 1\}$, a soft predicate quantifies the extent to which values of latent variables are consistent with the predicate.
This allows it to serve a role similar to a likelihood function, and opens up the use of likelihood-based inference procedures.
% TALK ABOUT REPLICA EXCHANGE.
% Conditioning is conventionally formalized as a restriction of a probability space to an event.
% To formalize soft conditioning, we generalize this to transformation of probability measure.

Predicates exist in a Boolean algebra; they can be conjoined, disjoined and negated.
This allows predicates to express knowledge that has complex Boolean structure.
Continuing the previous example, we may know that a person does \emph{not} have hypoglycemia, or that they have hypoglycemia \emph{or} hyperglycemia, or \emph{neither}.
To accommodate this, we construct a soft Boolean algebra, with continuous counterparts to equality, inequalities and logical connectives.
% Few likelihood-based likelihood-free approaches to inference offer effective means to condition non-trivial models on compound predicates.


% This degree to which a is is determined by a notion of distance.
% In contrast to most distance based inference methods (notably Approximate Bayesian Computation \cite{beaumont2002approximate}), we develop a form of replica exchange Markov Chain Monte Carlo \cite{earl2005parallel} to target inference that is exact in convergence of the chain.
% Predicate relaxation is modulated by temperature such that at zero temperature the relaxed predicate mirrors its hard counter-part, while at maximal temperatures, it is virtually always satisfied.
% Predicate Exchange simulates several Markov chains in parallel at different temperatures.



Predicate exchange is motivated by probabilistic programming languages, which have vastly expanded the class of probabilistic models that can be expressed, but still heavily restrict the kinds of predicates that can be conditioned on.
In a similar vein to  \cite{wingate2011lightweight} we provide a light-weight implementation that modulates the execution of a stochastic simulation based model to perform inference.
This means predicate exchange is easily incorporated into most frameworks. 

Our approach comes with certain limitations.
Equality conditions on continuous variables indicate sets of zero measure.
This is problematic because the probability of proposing a satisfying state in a Markov chain becomes zero.
In these cases predicate exchange must sample at a minimum temperature strictly greater than zero, which is approximate.
Another limitation occurs if a predicate has branches (e.g., if-then-else statements) which depend on uncertainty in the model.
In these cases, a soft predicate may underestimate the degree to which values of latent variables are consistent with the predicate, which can negatively effect inference in practice.
% Since all possible paths through a program are visited, our measure of is necessarily an estimate.

In summary, we:
% In detail, we:

\begin{enumerate}
	\item Formalize soft conditioning of simulation based  models in measure theoretic probability as a transformation of a probability measure (Section \ref{simmodels}).
	\item Motivate predicate relaxation (Section \ref{predexchange}), and provide a complete continuous Boolean algebra.
	\item Implement predicate exchange as nonstandard execution of a simulation based model (Section \ref{implement}).
	\item Evaluate our approach on examples, including a case study in glycemic forecasting (Section \ref{experiments}).
\end{enumerate}


% WTP: Contribution: inference algorithm that supports conditioning on a wider class of propositions
% In this paper we present an algorithm that draws samples from generative models that have been conditioned on predicates belonging to a more general class than observation of data.
% Predicates, when used as black boxes, provide only sparse information -- the constraint is satisfied or it is not -- and the subset of satisfying constraints is typically vanishingly small.
% Our objective is to support conditioning on predicates on spaces for which a natural metric can be defined.
% A metric provides more information a measure of the degree of satisfaction, and allows us.

% WTP: Paper summary
% In summary we address the problem of conditioning on declarative knowledge.
% In more detail:
% \begin{itemize}
% \item We formalize simulation models in measure-theoretic probability as random variables defined on a shared probability space (section X), and define conditioning as a concentration of measure.
% \item We describe our approach to inference, which softens the hard constraints to admit tractable inference in a broader set of scenarios.
% \item  We demonstrate our approach on a number of examples, with experiments on toy data and experiments on medical models by enriching them with declarative knowledge to learn from limited data.
% \end{itemize}