\section{Related Work}

% Likelihood free inference
Likelihood-free inference emerged in genetics ecology.
Tavar{\'e} et al. \yrcite{tavare1997inferring} 
filtered samples from a stochastic simulator to only those which matched (according to summary statistics) observed data. 
%  to perform one of the earliest forms of likelihood-free inference.
Weiss et al. \yrcite{weiss1998inference} extended this with a tolerance term, so that simulations sufficiently close to the data were accepted.
% Such posterior samples are therefore approximate.
A variety of approaches in this general regime  \cite{beaumont2002approximate,sisson2007sequential} fall under the heading of Approximate Bayesian Computation (ABC).
Marjoram et al. \yrcite{marjoram2003markov} simulated Markov Chains according to the prior, but applied the same summary statistic based filtering to yield approximate posterior samples.
A small tolerance leads to a high rate of rejected simulations, whereas a large tolerance results in an unacceptable approximation error.
Proposed solutions are dynamically decreasing the tolerance \cite{toni2008approximate}, importance reweighting samples based on distance \cite{wegmann2009efficient}, adapting the tolerance based on distance \cite{del2012adaptive,lenormand2013adaptive}, and annealing the tolerance \cite{albert2015simulated}.
Predicate exchange targets simulation models and uses distance metrics, but does not require summary statistics.

% A recent approach \cite{graham2017asymptotically}  with similar objectives develops a Hamiltonian Monte Carlo variant, using a quasi-Newton method during leap-frog integration to exactly solve the observation constraint.
% This is limited to differentiable models conditioned with equality.
% Predicate exchange is not limited to differentiable models, but must be approximate when the condition is of measure zero.

Existing work relaxing programs includes \cite{chaudhuri2010smooth} which extends Gaussian smoothing to programs, and \cite{ritchie2015generating} which defines soft-equality within an HMC sampler.
Predicate exchange is not bound to HMC, provides a complete soft algebra, and can sample from the true posterior in non-measure cases.

% PPLS
Probabilistic logics such as ProbLog \cite{richardson2006markov} and Markov logic networks \cite{de2007problog} extend first order logic with probabilities.
Probabilistic soft logic (PSL) \cite{brocheler2012probabilistic,kimmig2012short} uses continuous logic to encode graded beliefs.
For example, $\text{isfriend(Alice, Bob)} \to 0.9$ denotes a strong friendship between Alice and Bob.
In contrast, predicate exchange uses relaxation solely to make inference more tractable; soft Boolean values only exist within the sampling process and not in the resulting samples themselves.
In addition, predicate exchange is motivated by languages for generative models, such as ~\citep{milch20071, wood2014new,mansinghka2014venture,goodman2008church}, rather than logic based languages.


Several continuous \cite{levin2000continuous} and fuzzy \cite{klir1995fuzzy} logics apply model-theoretic tools to metric structures.
Continuous logics replace the Boolean structure $\{T, F\}$, quantifiers $\forall x$ and $\exists x$, and logical connectives with continuous counter-parts.
The main technical difference of our continuous logic is its two-sidedness, for negation. 
The main conceptual difference is that we use continuous logic only to increase the tractability of conventional inference.
% Talk about Soft logic



