\section{Related Work}

% Likelihood free inference
Likelihood-free inference emerged in genetics ecology.
Tavar{\'e} et al. \yrcite{tavare1997inferring} 
filtered samples from a stochastic simulator to only those which matched (according to summary statistics) observed data. 
%  to perform one of the earliest forms of likelihood-free inference.
Weiss et al. \yrcite{weiss1998inference} extended this with a tolerance term, so that simulations sufficiently close to the data were accepted.
% Such posterior samples are therefore approximate.
A variety of approaches in this general regime  \cite{beaumont2002approximate,sisson2007sequential} fall under the heading of Approximate Bayesian Computation (ABC).
Marjoram et al. \yrcite{marjoram2003markov} simulated Markov Chains according to the prior, but applied the same summary statistic based filtering to yield approximate posterior samples.
A small tolerance leads to a high rate of rejected simulations, whereas a large tolerance results in an unacceptable approximation error.
Proposed solutions are dynamically decreasing the tolerance \cite{toni2008approximate}, importance reweighting samples based on distance \cite{wegmann2009efficient}, adapting the tolerance based on distance \cite{del2012adaptive,lenormand2013adaptive}, and annealing the tolerance \cite{albert2015simulated}.
Predicate exchange targets simulation models and uses distance metrics, but does not require summary statistics.

% A recent approach \cite{graham2017asymptotically}  with similar objectives develops a Hamiltonian Monte Carlo variant, using a quasi-Newton method during leap-frog integration to exactly solve the observation constraint.
% This is limited to differentiable models conditioned with equality.
% Predicate exchange is not limited to differentiable models, but must be approximate when the condition is of measure zero.

% PPLS
Probabilistic logics such as ProbLog \cite{richardson2006markov} and Markov logic networks \cite{de2007problog} extend first order logic to express both models and conditions.
Of particular note, probabilistic soft logic (PSL) \cite{brocheler2012probabilistic,kimmig2012short} uses continuous logic to encode graded beliefs.
For example, $\text{isfriend(Alice, Bob)} \to 0.9$ denotes a strong friendship between Alice and Bob.
The primary distinction between PSL and predicate exchange is in the semantics of the soft predicates.
In predicate exchange, relaxation is used solely to make inference more tractable; soft Boolean values are used only within the sampling process and do not appear in the resulting samples themselves.
In addition, predicate exchange is motivated by probabilistic programming languages for generative models, such as ~\citep{milch20071, wood2014new,mansinghka2014venture,goodman2008church}, rather than purely declarative logic based languages.


Several continuous \cite{levin2000continuous} and fuzzy \cite{klir1995fuzzy} logics apply model-theoretic tools to metric structures.
Continuous logics replace the Boolean structure $\{T, F\}$, quantifiers $\forall x$ and $\exists x$, and logical connectives with continuous counter-parts.
The main technical difference of our continuous logic is its two-sidedness, for negation. 
The main conceptual difference is that we use continuous logic only to increase the tractability of conventional inference.
% Talk about Soft logic



